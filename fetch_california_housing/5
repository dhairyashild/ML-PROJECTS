# STEP 5: MODEL TRAINING & HYPERPARAMETER TUNING
# Stages: 5.1 Multiple Algorithms, 5.2 Hyperparameter Tuning, 5.3 Model Evaluation, 5.4 Best Model Selection
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Load processed data from Step 4
X_train = pd.read_csv('X_train_scaled.csv')
X_test = pd.read_csv('X_test_scaled.csv')
y_train = pd.read_csv('y_train.csv').squeeze()
y_test = pd.read_csv('y_test.csv').squeeze()

#### 5.1 MULTIPLE ALGORITHMS (COMPARE DIFFERENT MODELS)
print("‚úì 5.1 MULTIPLE ALGORITHM TRAINING")
# BEST PRACTICE: Try diverse algorithms to find best performer
# REAL-TIME: Linear models, tree-based models, and ensembles cover most cases
# AVOID: Using only one algorithm without comparison

models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'Support Vector Machine': SVR(kernel='rbf')
}

results = {}
print("Training multiple models...")
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    
    results[name] = {'RMSE': rmse, 'R2': r2, 'MAE': mae}
    print(f"{name:20} | RMSE: {rmse:.4f} | R¬≤: {r2:.4f} | MAE: {mae:.4f}")

#### 5.2 HYPERPARAMETER TUNING (OPTIMIZE BEST MODEL)
print("\n‚úì 5.2 HYPERPARAMETER TUNING")
# BEST PRACTICE: Tune only the most promising model to save time
# REAL-TIME: Gradient Boosting often performs best on structured data
# AVOID: Tuning all models - focus on the best one

# Select best model based on R¬≤ score
best_model_name = max(results, key=lambda x: results[x]['R2'])
print(f"Best model: {best_model_name} (R¬≤: {results[best_model_name]['R2']:.4f})")

# Hyperparameter tuning for Gradient Boosting
if best_model_name == 'Gradient Boosting':
    param_grid = {
        'n_estimators': [100, 200],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 4, 5],
        'subsample': [0.8, 0.9]
    }
    
    gb = GradientBoostingRegressor(random_state=42)
    grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    
    best_model = grid_search.best_estimator_
    print(f"Best parameters: {grid_search.best_params_}")
else:
    best_model = models[best_model_name]

#### 5.3 FINAL MODEL EVALUATION
print("\n‚úì 5.3 FINAL MODEL EVALUATION")
# BEST PRACTICE: Comprehensive evaluation on test set
# REAL-TIME: Use multiple metrics for complete picture
# AVOID: Relying only on one metric

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
final_r2 = r2_score(y_test, y_pred)
final_mae = mean_absolute_error(y_test, y_pred)

print("üîç FINAL MODEL PERFORMANCE:")
print(f"RMSE: {final_rmse:.4f}")
print(f"R¬≤ Score: {final_r2:.4f}")
print(f"MAE: {final_mae:.4f}")

# Cross-validation for robustness
cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='r2')
print(f"Cross-Validation R¬≤: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})")

#### 5.4 SAVE BEST MODEL
print("\n‚úì 5.4 SAVE BEST MODEL")
import joblib

# Save the trained model
joblib.dump(best_model, 'best_housing_model.pkl')
joblib.dump(scaler, 'scaler.pkl')  # Save scaler for future use

print("üíæ Best model saved as 'best_housing_model.pkl'")
print("üíæ Scaler saved as 'scaler.pkl'")

#### READY FOR STEP 6: MODEL DEPLOYMENT & MONITORING
# NEXT: Model interpretation, deployment, and continuous monitoring
