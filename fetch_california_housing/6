# STEP 6: MODEL DEPLOYMENT & MONITORING
# Stages: 6.1 Model Interpretation, 6.2 Prediction Pipeline, 6.3 Performance Monitoring, 6.4 Model Saving
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import shap
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Load best model and scaler from Step 5
best_model = joblib.load('best_housing_model.pkl')
scaler = joblib.load('scaler.pkl')

# Load test data for demonstration
X_test = pd.read_csv('X_test_scaled.csv')
y_test = pd.read_csv('y_test.csv').squeeze()

#### 6.1 ENTERPRISE-GRADE MODEL INTERPRETATION
print("‚úì 6.1 ENTERPRISE MODEL INTERPRETATION")
# PRODUCTION BEST: Multiple interpretation methods for different stakeholders
# REAL-TIME: SHAP for data scientists, feature importance for business users
# ENTERPRISE: Model cards for regulatory compliance

# Feature importance with confidence intervals (production standard)
if hasattr(best_model, 'feature_importances_'):
    # Calculate feature importance with bootstrap confidence intervals
    n_bootstraps = 100
    importances = np.zeros((n_bootstraps, len(X_test.columns)))
    
    for i in range(n_bootstraps):
        # Bootstrap sample
        indices = np.random.choice(range(len(X_test)), size=len(X_test), replace=True)
        X_boot = X_test.iloc[indices]
        y_boot = y_test.iloc[indices]
        
        # Retrain and get importance
        best_model.fit(X_boot, y_boot)
        if hasattr(best_model, 'feature_importances_'):
            importances[i] = best_model.feature_importances_
    
    # Calculate mean and confidence intervals
    importance_mean = importances.mean(axis=0)
    importance_std = importances.std(axis=0)
    importance_ci = 1.96 * importance_std / np.sqrt(n_bootstraps)
    
    # Create production-ready feature importance report
    feature_importance_df = pd.DataFrame({
        'feature': X_test.columns,
        'importance_mean': importance_mean,
        'importance_std': importance_std,
        'ci_lower': importance_mean - importance_ci,
        'ci_upper': importance_mean + importance_ci
    }).sort_values('importance_mean', ascending=False)
    
    print("Production Feature Importance with Confidence Intervals:")
    print(feature_importance_df.head(10).to_string())
    
    # Save for monitoring dashboard
    feature_importance_df.to_csv('production_feature_importance.csv', index=False)

# SHAP analysis with production optimizations
print("\nüîç Generating Production SHAP Explanations...")
try:
    # Use subset for faster computation in production
    sample_size = min(500, len(X_test))
    X_sample = X_test.sample(sample_size, random_state=42)
    
    # Initialize SHAP explainer
    explainer = shap.Explainer(best_model, X_sample)
    shap_values = explainer(X_sample)
    
    # Summary plot for production dashboard
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, show=False)
    plt.title('Production SHAP Feature Importance', fontsize=16)
    plt.tight_layout()
    plt.savefig('production_shap_summary.png', dpi=300, bbox_inches='tight')
    plt.close()  # Close to avoid display in automated environments
    
    # Individual prediction explanations (for API responses)
    example_idx = 0
    plt.figure(figsize=(10, 6))
    shap.waterfall_plot(shap_values[example_idx], show=False)
    plt.title(f'SHAP Explanation for Prediction #{example_idx}', fontsize=14)
    plt.tight_layout()
    plt.savefig('example_shap_explanation.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("‚úì SHAP analysis completed and saved for production dashboard")
    
except Exception as e:
    print(f"‚ö† SHAP analysis skipped: {str(e)}")

#### 6.2 PRODUCTION PREDICTION PIPELINE
print("\n‚úì 6.2 PRODUCTION PREDICTION PIPELINE")
# PRODUCTION BEST: Input validation, error handling, and telemetry
# REAL-TIME: Data quality checks and anomaly detection
# ENTERPRISE: Versioned pipeline with rollback capability

class HousingPricePredictor:
    """Production-grade prediction pipeline with validation and monitoring"""
    
    def __init__(self, model, scaler, feature_names):
        self.model = model
        self.scaler = scaler
        self.feature_names = feature_names
        self.prediction_log = []
        
        # Load expected data ranges from training (for validation)
        X_train = pd.read_csv('X_train_scaled.csv')
        self.feature_ranges = {
            col: {
                'min': float(X_train[col].min()),
                'max': float(X_train[col].max()),
                'mean': float(X_train[col].mean())
            } for col in feature_names
        }
    
    def validate_input(self, input_dict):
        """Validate input data against expected ranges"""
        warnings = []
        for feature, value in input_dict.items():
            if feature in self.feature_ranges:
                feat_range = self.feature_ranges[feature]
                if value < feat_range['min'] * 0.5 or value > feat_range['max'] * 1.5:
                    warnings.append(f"{feature} value {value} outside expected range [{feat_range['min']:.2f}, {feat_range['max']:.2f}]")
        return warnings
    
    def predict(self, features_dict, return_confidence=False):
        """
        Production prediction with validation and logging
        Input: Dictionary of feature values
        Output: Predicted house price with optional confidence
        """
        try:
            # Validate input
            validation_warnings = self.validate_input(features_dict)
            
            # Convert to DataFrame with correct column order
            input_df = pd.DataFrame([features_dict])[self.feature_names]
            
            # Apply scaling
            input_scaled = self.scaler.transform(input_df)
            
            # Make prediction
            prediction = self.model.predict(input_scaled)[0]
            
            # Calculate simple confidence score based on input validity
            confidence = 0.9  # Base confidence
            if validation_warnings:
                confidence = max(0.5, confidence - len(validation_warnings) * 0.1)
            
            # Log prediction for monitoring
            self.log_prediction(features_dict, prediction, validation_warnings)
            
            if return_confidence:
                return round(prediction, 4), round(confidence, 2), validation_warnings
            else:
                return round(prediction, 4), validation_warnings
                
        except Exception as e:
            # Log error for monitoring
            error_msg = f"Prediction error: {str(e)}"
            self.log_prediction(features_dict, None, [error_msg])
            raise ValueError(error_msg)
    
    def log_prediction(self, features, prediction, warnings):
        """Log prediction for monitoring and analytics"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'features': features,
            'prediction': prediction,
            'warnings': warnings,
            'model_version': '1.0.0'
        }
        self.prediction_log.append(log_entry)
        
        # Keep only last 1000 entries in memory
        if len(self.prediction_log) > 1000:
            self.prediction_log = self.prediction_log[-1000:]
    
    def get_prediction_stats(self):
        """Get statistics for monitoring dashboard"""
        return {
            'total_predictions': len(self.prediction_log),
            'recent_predictions': len([p for p in self.prediction_log 
                                     if datetime.fromisoformat(p['timestamp']) > datetime.now().replace(hour=0, minute=0, second=0)]),
            'warning_rate': len([p for p in self.prediction_log if p['warnings']]) / len(self.prediction_log) if self.prediction_log else 0
        }

# Initialize production predictor
predictor = HousingPricePredictor(best_model, scaler, X_test.columns.tolist())

# Test the production pipeline
sample_input = {
    'MedInc': 3.5,
    'HouseAge': 25,
    'AveRooms': 5.5,
    'AveBedrms': 1.1,
    'Population': 1200,
    'AveOccup': 2.8,
    'Latitude': 34.5,
    'Longitude': -118.2,
    'RoomsPerHousehold': 2.1,
    'IncomePerRoom': 0.7,
    'BedroomRatio': 0.2,
    'PopulationDensity': 450
}

try:
    predicted_price, confidence, warnings = predictor.predict(sample_input, return_confidence=True)
    print(f"Predicted house price: ${predicted_price * 100000:,.0f}")
    print(f"Confidence score: {confidence}")
    if warnings:
        print(f"Warnings: {warnings}")
except Exception as e:
    print(f"Prediction failed: {e}")

#### 6.3 ENTERPRISE PERFORMANCE MONITORING
print("\n‚úì 6.3 ENTERPRISE PERFORMANCE MONITORING")
# PRODUCTION BEST: Comprehensive monitoring with alerting
# REAL-TIME: Data drift, concept drift, and performance degradation
# ENTERPRISE: Integration with monitoring tools like Prometheus, Datadog

class ProductionMonitor:
    """Enterprise-grade model performance monitoring"""
    
    def __init__(self):
        self.performance_history = []
        self.drift_metrics = []
    
    def calculate_performance_metrics(self, y_true, y_pred):
        """Calculate comprehensive performance metrics"""
        return {
            'timestamp': datetime.now().isoformat(),
            'rmse': float(np.sqrt(mean_squared_error(y_true, y_pred))),
            'mae': float(mean_absolute_error(y_true, y_pred)),
            'r2': float(r2_score(y_true, y_pred)),
            'max_error': float(max(abs(y_true - y_pred))),
            'median_error': float(np.median(abs(y_true - y_pred)))
        }
    
    def check_data_drift(self, current_data, reference_data):
        """Check for data drift using population stability index"""
        drift_scores = {}
        for col in current_data.columns:
            # Simple drift detection (PSI-like)
            current_mean = current_data[col].mean()
            reference_mean = reference_data[col].mean()
            drift = abs(current_mean - reference_mean) / reference_mean
            drift_scores[col] = drift
        
        return drift_scores
    
    def generate_monitoring_report(self, y_true, y_pred, current_data):
        """Generate comprehensive monitoring report"""
        # Calculate performance metrics
        metrics = self.calculate_performance_metrics(y_true, y_pred)
        
        # Check for data drift
        reference_data = pd.read_csv('X_train_scaled.csv')
        drift_scores = self.check_data_drift(current_data, reference_data)
        
        # Check for performance degradation
        baseline_performance = 0.75  # From initial training
        performance_degradation = baseline_performance - metrics['r2']
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'performance_metrics': metrics,
            'data_drift_scores': drift_scores,
            'performance_degradation': performance_degradation,
            'alerts': self.generate_alerts(metrics, drift_scores, performance_degradation)
        }
        
        # Save to history
        self.performance_history.append(report)
        return report
    
    def generate_alerts(self, metrics, drift_scores, performance_degradation):
        """Generate alerts based on thresholds"""
        alerts = []
        
        # Performance alerts
        if metrics['r2'] < 0.6:
            alerts.append(f"CRITICAL: R¬≤ dropped to {metrics['r2']:.3f}")
        elif performance_degradation > 0.1:
            alerts.append(f"WARNING: Performance degradation {performance_degradation:.3f}")
        
        # Data drift alerts
        high_drift_features = [f for f, score in drift_scores.items() if score > 0.15]
        if high_drift_features:
            alerts.append(f"DATA DRIFT: Features with high drift: {high_drift_features}")
        
        return alerts

# Initialize monitor and generate report
monitor = ProductionMonitor()
current_performance = monitor.calculate_performance_metrics(y_test, best_model.predict(X_test))
drift_scores = monitor.check_data_drift(X_test, pd.read_csv('X_train_scaled.csv'))

print("üìä Production Monitoring Dashboard:")
print(f"Current R¬≤: {current_performance['r2']:.3f}")
print(f"Current RMSE: {current_performance['rmse']:.3f}")
print(f"Worst data drift: {max(drift_scores.values()):.3f}")

# Generate full report
monitoring_report = monitor.generate_monitoring_report(y_test, best_model.predict(X_test), X_test)
print(f"Active alerts: {len(monitoring_report['alerts'])}")

#### 6.4 ENTERPRISE MODEL DEPLOYMENT PACKAGE
print("\n‚úì 6.4 ENTERPRISE DEPLOYMENT PACKAGE")
# PRODUCTION BEST: Complete deployment artifacts with documentation
# REAL-TIME: Versioned packages with dependency management
# ENTERPRISE: Model cards, validation schemas, and monitoring config

# Create model card for responsible AI
model_card = {
    "model_details": {
        "name": "California Housing Price Predictor",
        "version": "1.0.0",
        "description": "Gradient Boosting model for predicting median house values in California",
        "training_data": "California Housing Dataset from sklearn",
        "training_date": datetime.now().strftime("%Y-%m-%d"),
        "performance_metrics": current_performance
    },
    "considerations": {
        "intended_use": ["Real estate valuation", "Market analysis", "Investment analysis"],
        "limitations": ["Geographically limited to California", "Based on 1990 census data"],
        "ethical_considerations": ["Potential for reinforcing existing biases", "Should not be sole factor in housing decisions"],
        "fairness_analysis": {
            "geographic_fairness": "Trained on statewide data but performance may vary by region",
            "economic_fairness": "May perform differently across income brackets"
        }
    },
    "technical_details": {
        "model_architecture": type(best_model).__name__,
        "hyperparameters": best_model.get_params() if hasattr(best_model, 'get_params') else {},
        "feature_importance": feature_importance_df.to_dict('records') if 'feature_importance_df' in locals() else []
    }
}

with open('model_card.json', 'w') as f:
    json.dump(model_card, f, indent=2)

# Create validation schema for production data validation
validation_schema = {
    "feature_constraints": {
        col: {
            "min_value": float(X_test[col].min()),
            "max_value": float(X_test[col].max()),
            "expected_range": [float(X_test[col].quantile(0.01)), float(X_test[col].quantile(0.99))],
            "data_type": str(X_test[col].dtype)
        } for col in X_test.columns
    },
    "quality_thresholds": {
        "max_missing_percentage": 0.05,
        "outlier_threshold": 3.0,  # Z-score
        "drift_alert_threshold": 0.15
    }
}

with open('validation_schema.json', 'w') as f:
    json.dump(validation_schema, f, indent=2)

# Save complete deployment package
deployment_package = {
    'model': best_model,
    'scaler': scaler,
    'predictor': predictor,
    'monitor': monitor,
    'feature_names': X_test.columns.tolist(),
    'model_version': '1.0.0',
    'deployment_date': datetime.now().isoformat(),
    'model_card': model_card,
    'validation_schema': validation_schema,
    'performance_baseline': current_performance
}

joblib.dump(deployment_package, 'production_deployment_package.pkl')

print("üöÄ Enterprise deployment package saved:")
print("   - Model, scaler, and production utilities")
print("   - Model card for responsible AI")
print("   - Validation schema for data quality")
print("   - Performance baseline metrics")

print("\n" + "="*70)
print("üéØ ENTERPRISE DEPLOYMENT READY!")
print("Next steps:")
print("1. Containerize with Docker: Build API service with FastAPI/Flask")
print("2. Deploy to cloud platform (AWS SageMaker, Azure ML, GCP AI Platform)")
print("3. Set up monitoring with Prometheus/Grafana or cloud-native tools")
print("4. Implement CI/CD pipeline for automated retraining")
print("5. Create A/B testing framework for model updates")
print("="*70)
